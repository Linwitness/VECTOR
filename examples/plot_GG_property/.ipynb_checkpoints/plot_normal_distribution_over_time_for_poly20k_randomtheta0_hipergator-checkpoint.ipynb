{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61989128-982f-4be3-b9ee-f61999e5d1b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'myInput'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/scratch/local/21179043/ipykernel_3796283/1424680821.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/../../'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/../calculate_tangent/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmyInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPACKAGE_MP_Linear\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlinear2d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'myInput'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Jul 31 14:33:57 2023\n",
    "\n",
    "@author: Lin\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "current_path = os.getcwd()\n",
    "import numpy as np\n",
    "from numpy import seterr\n",
    "seterr(all='raise')\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append(current_path)\n",
    "sys.path.append(current_path+'/../../')\n",
    "sys.path.append(current_path+'/../calculate_tangent/')\n",
    "import myInput\n",
    "import PACKAGE_MP_Linear as linear2d\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc316a01-0f2f-40a9-9fa2-425de11bfdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simple_magnitude(freqArray):\n",
    "    xLim = [0, 360]\n",
    "    binValue = 10.01\n",
    "    binNum = round((abs(xLim[0])+abs(xLim[1]))/binValue)\n",
    "    xCor = np.linspace((xLim[0]+binValue/2),(xLim[1]-binValue/2),binNum)\n",
    "\n",
    "    freqArray_circle = np.ones(binNum)\n",
    "    freqArray_circle = freqArray_circle/sum(freqArray_circle*binValue)\n",
    "\n",
    "    magnitude_max = np.max(abs(freqArray - freqArray_circle))/np.average(freqArray_circle)\n",
    "    magnitude_ave = np.average(abs(freqArray - freqArray_circle))/np.average(freqArray_circle)\n",
    "\n",
    "    magnitude_stan = np.sqrt(np.sum((abs(freqArray - freqArray_circle)/np.average(freqArray_circle) - magnitude_ave)**2)/binNum)\n",
    "\n",
    "    return magnitude_ave, magnitude_stan\n",
    "\n",
    "    # coeff_high = abs(np.cos((xCor-90)/180*np.pi))\n",
    "    # coeff_low = abs(np.cos((xCor)/180*np.pi))\n",
    "    # return np.sum(freqArray * coeff_high)/np.sum(freqArray * coeff_low)\n",
    "\n",
    "def find_fittingEllipse2(array): #failure\n",
    "    K_mat = []\n",
    "    Y_mat = []\n",
    "\n",
    "    # Get the self-variable\n",
    "    X = array[:,0]\n",
    "    Y = array[:,1]\n",
    "\n",
    "    K_mat = np.hstack([X**2, X*Y, Y**2, X, Y])\n",
    "    Y_mat = np.ones_like(X)\n",
    "\n",
    "    X_mat = np.linalg.lstsq(K_mat, Y_mat)[0].squeeze()\n",
    "    # X_mat = (K_mat.T*K_mat).I * K_mat.T * Y_mat\n",
    "\n",
    "    print('The ellipse is given by {0:.3}x^2 + {1:.3}xy+{2:.3}y^2+{3:.3}x+{4:.3}y = 1'.format(X_mat[0], X_mat[1], X_mat[2], X_mat[3], X_mat[4]))\n",
    "    print(X_mat)\n",
    "\n",
    "    return X_mat\n",
    "\n",
    "def get_poly_center(micro_matrix, step):\n",
    "    # Get the center of all non-periodic grains in matrix\n",
    "    num_grains = int(np.max(micro_matrix[step,:]))\n",
    "    center_list = np.zeros((num_grains,2))\n",
    "    sites_num_list = np.zeros(num_grains)\n",
    "    ave_radius_list = np.zeros(num_grains)\n",
    "    coord_refer_i = np.zeros((micro_matrix.shape[1], micro_matrix.shape[2]))\n",
    "    coord_refer_j = np.zeros((micro_matrix.shape[1], micro_matrix.shape[2]))\n",
    "    for i in range(micro_matrix.shape[1]):\n",
    "        for j in range(micro_matrix.shape[2]):\n",
    "            coord_refer_i[i,j] = i\n",
    "            coord_refer_j[i,j] = j\n",
    "\n",
    "    table = micro_matrix[step,:,:,0]\n",
    "    for i in range(num_grains):\n",
    "        sites_num_list[i] = np.sum(table == i+1)\n",
    "\n",
    "        if (sites_num_list[i] < 500) or \\\n",
    "           (np.max(coord_refer_i[table == i+1]) - np.min(coord_refer_i[table == i+1]) == micro_matrix.shape[1]) or \\\n",
    "           (np.max(coord_refer_j[table == i+1]) - np.min(coord_refer_j[table == i+1]) == micro_matrix.shape[2]): # grains on bc are ignored\n",
    "          center_list[i, 0] = 0\n",
    "          center_list[i, 1] = 0\n",
    "          sites_num_list[i] == 0\n",
    "        else:\n",
    "          center_list[i, 0] = np.sum(coord_refer_i[table == i+1]) / sites_num_list[i]\n",
    "          center_list[i, 1] = np.sum(coord_refer_j[table == i+1]) / sites_num_list[i]\n",
    "    ave_radius_list = np.sqrt(sites_num_list / np.pi)\n",
    "\n",
    "    return center_list, ave_radius_list\n",
    "\n",
    "def get_poly_statistical_radius(micro_matrix, sites_list, step):\n",
    "    # Get the max offset of average radius and real radius\n",
    "    center_list, ave_radius_list = get_poly_center(micro_matrix, step)\n",
    "    num_grains = int(np.max(micro_matrix[step,:]))\n",
    "\n",
    "    max_radius_offset_list = np.zeros(num_grains)\n",
    "    for n in range(num_grains):\n",
    "        center = center_list[n]\n",
    "        ave_radius = ave_radius_list[n]\n",
    "        sites = sites_list[n]\n",
    "\n",
    "        if ave_radius != 0:\n",
    "          for sitei in sites:\n",
    "              [i,j] = sitei\n",
    "              current_radius = np.sqrt((i - center[0])**2 + (j - center[1])**2)\n",
    "              radius_offset = abs(current_radius - ave_radius)\n",
    "              if radius_offset > max_radius_offset_list[n]: max_radius_offset_list[n] = radius_offset\n",
    "\n",
    "          max_radius_offset_list[n] = max_radius_offset_list[n] / ave_radius\n",
    "\n",
    "    max_radius_offset = np.average(max_radius_offset_list[max_radius_offset_list!=0])\n",
    "    area_list = np.pi*ave_radius_list*ave_radius_list\n",
    "    if np.sum(area_list) == 0: max_radius_offset = 0\n",
    "    else: max_radius_offset = np.sum(max_radius_offset_list * area_list) / np.sum(area_list)\n",
    "\n",
    "    return max_radius_offset\n",
    "\n",
    "def get_poly_statistical_ar(micro_matrix, step):\n",
    "    # Get the average aspect ratio\n",
    "    num_grains = int(np.max(micro_matrix[step,:]))\n",
    "    sites_num_list = np.zeros(num_grains)\n",
    "    coord_refer_i = np.zeros((micro_matrix.shape[1], micro_matrix.shape[2]))\n",
    "    coord_refer_j = np.zeros((micro_matrix.shape[1], micro_matrix.shape[2]))\n",
    "    for i in range(micro_matrix.shape[1]):\n",
    "        for j in range(micro_matrix.shape[2]):\n",
    "            coord_refer_i[i,j] = i\n",
    "            coord_refer_j[i,j] = j\n",
    "\n",
    "    aspect_ratio_i = np.zeros((num_grains,2))\n",
    "    aspect_ratio_j = np.zeros((num_grains,2))\n",
    "    aspect_ratio = np.zeros(num_grains)\n",
    "    table = micro_matrix[step,:,:,0]\n",
    "\n",
    "    aspect_ratio_i_list = [[] for _ in range(int(num_grains))]\n",
    "    aspect_ratio_j_list = [[] for _ in range(int(num_grains))]\n",
    "    for i in range(micro_matrix.shape[1]):\n",
    "        for j in range(micro_matrix.shape[2]):\n",
    "            grain_id = int(table[i][j]-1)\n",
    "            sites_num_list[grain_id] +=1\n",
    "            aspect_ratio_i_list[grain_id].append(coord_refer_i[i][j])\n",
    "            aspect_ratio_j_list[grain_id].append(coord_refer_j[i][j])\n",
    "\n",
    "    for i in range(num_grains):\n",
    "        aspect_ratio_i[i, 0] = len(list(set(aspect_ratio_i_list[i])))\n",
    "        aspect_ratio_j[i, 1] = len(list(set(aspect_ratio_j_list[i])))\n",
    "        if aspect_ratio_j[i, 1] == 0: aspect_ratio[i] = 0\n",
    "        else: aspect_ratio[i] = aspect_ratio_i[i, 0] / aspect_ratio_j[i, 1]\n",
    "\n",
    "    # aspect_ratio = np.average(aspect_ratio[aspect_ratio!=0])\n",
    "    aspect_ratio = np.sum(aspect_ratio * sites_num_list) / np.sum(sites_num_list)\n",
    "\n",
    "    return aspect_ratio\n",
    "\n",
    "def get_normal_vector(grain_structure_figure_one, grain_num):\n",
    "    nx = grain_structure_figure_one.shape[0]\n",
    "    ny = grain_structure_figure_one.shape[1]\n",
    "    ng = np.max(grain_structure_figure_one)\n",
    "    cores = 32\n",
    "    loop_times = 5\n",
    "    P0 = grain_structure_figure_one\n",
    "    R = np.zeros((nx,ny,2))\n",
    "    smooth_class = linear2d.linear_class(nx,ny,ng,cores,loop_times,P0,R)\n",
    "\n",
    "    smooth_class.linear_main(\"inclination\")\n",
    "    P = smooth_class.get_P()\n",
    "    # sites = smooth_class.get_gb_list(1)\n",
    "    # print(len(sites))\n",
    "    # for id in range(2,grain_num+1): sites += smooth_class.get_gb_list(id)\n",
    "    # print(len(sites))\n",
    "    sites = smooth_class.get_all_gb_list()\n",
    "    sites_together = []\n",
    "    for id in range(len(sites)): sites_together += sites[id]\n",
    "    print(\"Total num of GB sites: \" + str(len(sites_together)))\n",
    "\n",
    "    return P, sites_together, sites\n",
    "\n",
    "def get_normal_vector_slope(P, sites, step, para_name, bias=None):\n",
    "    xLim = [0, 360]\n",
    "    binValue = 10.01\n",
    "    binNum = round((abs(xLim[0])+abs(xLim[1]))/binValue)\n",
    "    xCor = np.linspace((xLim[0]+binValue/2),(xLim[1]-binValue/2),binNum)\n",
    "\n",
    "    freqArray = np.zeros(binNum)\n",
    "    degree = []\n",
    "    for sitei in sites:\n",
    "        [i,j] = sitei\n",
    "        dx,dy = myInput.get_grad(P,i,j)\n",
    "        degree.append(math.atan2(-dy, dx) + math.pi)\n",
    "        # if dx == 0:\n",
    "        #     degree.append(math.pi/2)\n",
    "        # elif dy >= 0:\n",
    "        #     degree.append(abs(math.atan(-dy/dx)))\n",
    "        # elif dy < 0:\n",
    "        #     degree.append(abs(math.atan(dy/dx)))\n",
    "    for i in range(len(degree)):\n",
    "        freqArray[int((degree[i]/math.pi*180-xLim[0])/binValue)] += 1\n",
    "    freqArray = freqArray/sum(freqArray*binValue)\n",
    "\n",
    "    if bias is not None:\n",
    "        freqArray = freqArray + bias\n",
    "        freqArray = freqArray/sum(freqArray*binValue)\n",
    "    # Plot\n",
    "    # plt.close()\n",
    "    # fig = plt.figure(figsize=(5, 5))\n",
    "    # ax = plt.gca(projection='polar')\n",
    "\n",
    "    # ax.set_thetagrids(np.arange(0.0, 360.0, 20.0),fontsize=14)\n",
    "    # ax.set_thetamin(0.0)\n",
    "    # ax.set_thetamax(360.0)\n",
    "\n",
    "    # ax.set_rgrids(np.arange(0, 0.008, 0.004))\n",
    "    # ax.set_rlabel_position(0.0)  # 标签显示在0°\n",
    "    # ax.set_rlim(0.0, 0.008)  # 标签范围为[0, 5000)\n",
    "    # ax.set_yticklabels(['0', '0.004'],fontsize=14)\n",
    "\n",
    "    # ax.grid(True, linestyle=\"-\", color=\"k\", linewidth=0.5, alpha=0.5)\n",
    "    # ax.set_axisbelow('True')\n",
    "    plt.plot(np.append(xCor,xCor[0])/180*math.pi, np.append(freqArray,freqArray[0]), linewidth=2, label=para_name)\n",
    "\n",
    "    # fitting\n",
    "    fit_coeff = np.polyfit(xCor, freqArray, 1)\n",
    "    return freqArray\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # File name\n",
    "    npy_file_folder = \"/blue/michael.tonks/lin.yang/SPPARKS-VirtualIncEnergy/2d_poly_multiCoreCompare/results/\"\n",
    "    TJ_energy_type_ave = \"ave\"\n",
    "    TJ_energy_type_consMin = \"consMin\"\n",
    "    TJ_energy_type_sum = \"sum\"\n",
    "    TJ_energy_type_min = \"min\"\n",
    "    TJ_energy_type_max = \"max\"\n",
    "    TJ_energy_type_consMax = \"consMax\"\n",
    "\n",
    "\n",
    "    npy_file_name_iso = \"p_ori_ave_aveE_20000_multiCore32_delta0.0_m2_J1_refer_1_0_0_seed56689_kt066.npy\"\n",
    "    npy_file_name_aniso_ave = f\"p_randomtheta0_{TJ_energy_type_ave}E_20000_delta0.6_J1_refer_1_0_0_seed56689_kt0.66.npy\"\n",
    "    # npy_file_name_aniso_consMin = f\"pm_randomtheta0_{TJ_energy_type_consMin}E_20000_delta0.6_J1_refer_1_0_0_seed56689_kt0.66.npy\"\n",
    "    # npy_file_name_aniso_sum = f\"pm_randomtheta0_{TJ_energy_type_sum}E_20000_delta0.6_J1_refer_1_0_0_seed56689_kt0.66.npy\"\n",
    "    npy_file_name_aniso_min = f\"p_randomtheta0_{TJ_energy_type_min}E_20000_delta0.6_J1_refer_1_0_0_seed56689_kt0.66.npy\"\n",
    "    npy_file_name_aniso_max = f\"p_randomtheta0_{TJ_energy_type_max}E_20000_delta0.6_J1_refer_1_0_0_seed56689_kt0.66.npy\"\n",
    "    # npy_file_name_aniso_consMax = f\"pm_randomtheta0_{TJ_energy_type_consMax}E_20000_delta0.6_J1_refer_1_0_0_seed56689_kt0.66.npy\"\n",
    "\n",
    "    # Initial data\n",
    "    npy_file_iso = np.load(npy_file_folder + npy_file_name_iso)\n",
    "    npy_file_aniso_ave = np.load(npy_file_folder + npy_file_name_aniso_ave)\n",
    "    # npy_file_aniso_consMin = np.load(npy_file_folder + npy_file_name_aniso_consMin)\n",
    "    # npy_file_aniso_sum = np.load(npy_file_folder + npy_file_name_aniso_sum)\n",
    "    npy_file_aniso_min = np.load(npy_file_folder + npy_file_name_aniso_min)\n",
    "    npy_file_aniso_max = np.load(npy_file_folder + npy_file_name_aniso_max)\n",
    "    # npy_file_aniso_consMax = np.load(npy_file_folder + npy_file_name_aniso_consMax)\n",
    "    print(f\"The ave data size is: {npy_file_aniso_ave.shape}\")\n",
    "    # print(f\"The consMin data size is: {npy_file_aniso_consMin.shape}\")\n",
    "    # print(f\"The sum data size is: {npy_file_aniso_sum.shape}\")\n",
    "    print(f\"The min data size is: {npy_file_aniso_min.shape}\")\n",
    "    print(f\"The max data size is: {npy_file_aniso_max.shape}\")\n",
    "    # print(f\"The consMax data size is: {npy_file_aniso_consMax.shape}\")\n",
    "    print(f\"The iso data size is: {npy_file_iso.shape}\")\n",
    "    print(\"READING DATA DONE\")\n",
    "\n",
    "    # Initial container\n",
    "    initial_grain_num = 20000\n",
    "    step_num = npy_file_aniso_ave.shape[0]\n",
    "\n",
    "    bin_width = 0.16 # Grain size distribution\n",
    "    x_limit = [-0.5, 3.5]\n",
    "    bin_num = round((abs(x_limit[0])+abs(x_limit[1]))/bin_width)\n",
    "    size_coordination = np.linspace((x_limit[0]+bin_width/2),(x_limit[1]-bin_width/2),bin_num)\n",
    "    grain_size_distribution_iso = np.zeros(bin_num)\n",
    "    special_step_distribution_iso = 10#to get 2000 grains\n",
    "    grain_size_distribution_ave = np.zeros(bin_num)\n",
    "    special_step_distribution_ave = 12 #to get 2000 grains\n",
    "    # grain_size_distribution_consMin = np.zeros(bin_num)\n",
    "    # special_step_distribution_consMin = 12#to get 2000 grains\n",
    "    # grain_size_distribution_sum = np.zeros(bin_num)\n",
    "    # special_step_distribution_sum = 12#to get 2000 grains\n",
    "    grain_size_distribution_iso = np.zeros(bin_num)\n",
    "    grain_size_distribution_min = np.zeros(bin_num)\n",
    "    special_step_distribution_min = 22#to get 2000 grains\n",
    "    grain_size_distribution_max = np.zeros(bin_num)\n",
    "    special_step_distribution_max = 12#to get 2000 grains\n",
    "    # grain_size_distribution_consMax = np.zeros(bin_num)\n",
    "    # special_step_distribution_consMax = 13#to get 2000 grains\n",
    "\n",
    "\n",
    "    # Get bias from kT test\n",
    "    special_step_distribution_T066_bias = 10\n",
    "    data_file_name_bias = f'/normal_distribution_data/normal_distribution_T066_bias_sites_step{special_step_distribution_T066_bias}.npy'\n",
    "    if os.path.exists(current_path + data_file_name_bias):\n",
    "        slope_list_bias = np.load(current_path + data_file_name_bias)\n",
    "    else:\n",
    "        data_file_name_P = f'/normal_distribution_data/normal_distribution_iso_P_step{special_step_distribution_T066_bias}.npy'\n",
    "        data_file_name_sites = f'/normal_distribution_data/normal_distribution_iso_sites_step{special_step_distribution_T066_bias}.npy'\n",
    "        if os.path.exists(current_path + data_file_name_P):\n",
    "            P = np.load(current_path + data_file_name_P)\n",
    "            sites = np.load(current_path + data_file_name_sites)\n",
    "        else:\n",
    "            newplace = np.rot90(npy_file_iso[special_step_distribution_T066_bias,:,:,:], 1, (0,1))\n",
    "            P, sites, sites_list = get_normal_vector(newplace, initial_grain_num)\n",
    "            np.save(current_path + data_file_name_P, P)\n",
    "            np.save(current_path + data_file_name_sites, sites)\n",
    "\n",
    "        plt.close()\n",
    "        fig = plt.figure(figsize=(5, 5))\n",
    "        ax = plt.gca(projection='polar')\n",
    "        slope_list = get_normal_vector_slope(P, sites, special_step_distribution_T066_bias, \"Iso\")\n",
    "        # For bias\n",
    "        xLim = [0, 360]\n",
    "        binValue = 10.01\n",
    "        binNum = round((abs(xLim[0])+abs(xLim[1]))/binValue)\n",
    "        freqArray_circle = np.ones(binNum)\n",
    "        freqArray_circle = freqArray_circle/sum(freqArray_circle*binValue)\n",
    "        slope_list_bias = freqArray_circle - slope_list\n",
    "        np.save(current_path + data_file_name_bias, slope_list_bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1102b6f-a2c9-4264-99a4-0a26d9c6c202",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    # Start polar figure\n",
    "    plt.close()\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    ax = plt.gca(projection='polar')\n",
    "\n",
    "    ax.set_thetagrids(np.arange(0.0, 360.0, 45.0),fontsize=16)\n",
    "    ax.set_thetamin(0.0)\n",
    "    ax.set_thetamax(360.0)\n",
    "\n",
    "    ax.set_rgrids(np.arange(0, 0.01, 0.004))\n",
    "    ax.set_rlabel_position(0.0)  # 标签显示在0°\n",
    "    ax.set_rlim(0.0, 0.01)  # 标签范围为[0, 5000)\n",
    "    ax.set_yticklabels(['0', '4e-3', '8e-3'],fontsize=16)\n",
    "\n",
    "    ax.grid(True, linestyle=\"-\", color=\"k\", linewidth=0.5, alpha=0.5)\n",
    "    ax.set_axisbelow('True')\n",
    "\n",
    "    \n",
    "\n",
    "    aniso_mag = np.zeros(6)\n",
    "    aniso_mag_stand = np.zeros(6)\n",
    "\n",
    "    # Aniso - min\n",
    "    data_file_name_P = f'/normal_distribution_data/normal_distribution_rdt0_min_P_step{special_step_distribution_min}.npy'\n",
    "    data_file_name_sites = f'/normal_distribution_data/normal_distribution_rdt0_min_sites_step{special_step_distribution_min}.npy'\n",
    "    if os.path.exists(current_path + data_file_name_P):\n",
    "        P = np.load(current_path + data_file_name_P)\n",
    "        sites = np.load(current_path + data_file_name_sites)\n",
    "    else:\n",
    "        newplace = np.rot90(npy_file_aniso_min[special_step_distribution_min,:,:,:], 1, (0,1))\n",
    "        P, sites, sites_list = get_normal_vector(newplace, initial_grain_num)\n",
    "        np.save(current_path + data_file_name_P, P)\n",
    "        np.save(current_path + data_file_name_sites, sites)\n",
    "\n",
    "    slope_list = get_normal_vector_slope(P, sites, special_step_distribution_min, \"Min\",slope_list_bias)\n",
    "    aniso_mag[0], aniso_mag_stand[0] = simple_magnitude(slope_list)\n",
    "\n",
    "    # Aniso - max\n",
    "    data_file_name_P = f'/normal_distribution_data/normal_distribution_rdt0_max_P_step{special_step_distribution_max}.npy'\n",
    "    data_file_name_sites = f'/normal_distribution_data/normal_distribution_rdt0_max_sites_step{special_step_distribution_max}.npy'\n",
    "    if os.path.exists(current_path + data_file_name_P):\n",
    "        P = np.load(current_path + data_file_name_P)\n",
    "        sites = np.load(current_path + data_file_name_sites)\n",
    "    else:\n",
    "        newplace = np.rot90(npy_file_aniso_max[special_step_distribution_max,:,:,:], 1, (0,1))\n",
    "        P, sites, sites_list = get_normal_vector(newplace, initial_grain_num)\n",
    "        np.save(current_path + data_file_name_P, P)\n",
    "        np.save(current_path + data_file_name_sites, sites)\n",
    "\n",
    "    slope_list = get_normal_vector_slope(P, sites, special_step_distribution_max, \"Max\",slope_list_bias)\n",
    "    aniso_mag[1], aniso_mag_stand[1] = simple_magnitude(slope_list)\n",
    "\n",
    "    # Aniso - ave\n",
    "    data_file_name_P = f'/normal_distribution_data/normal_distribution_rdt0_ave_P_step{special_step_distribution_ave}.npy'\n",
    "    data_file_name_sites = f'/normal_distribution_data/normal_distribution_rdt0_ave_sites_step{special_step_distribution_ave}.npy'\n",
    "    if os.path.exists(current_path + data_file_name_P):\n",
    "        P = np.load(current_path + data_file_name_P)\n",
    "        sites = np.load(current_path + data_file_name_sites)\n",
    "    else:\n",
    "        newplace = np.rot90(npy_file_aniso_ave[special_step_distribution_ave,:,:,:], 1, (0,1))\n",
    "        P, sites, sites_list = get_normal_vector(newplace, initial_grain_num)\n",
    "        np.save(current_path + data_file_name_P, P)\n",
    "        np.save(current_path + data_file_name_sites, sites)\n",
    "\n",
    "    slope_list = get_normal_vector_slope(P, sites, special_step_distribution_ave, \"Ave\",slope_list_bias)\n",
    "    aniso_mag[2], aniso_mag_stand[2] = simple_magnitude(slope_list)\n",
    "\n",
    "#     # Aniso - sum\n",
    "#     data_file_name_P = f'/normal_distribution_data/normal_distribution_rdt0m_sum_P_step{special_step_distribution_sum}.npy'\n",
    "#     data_file_name_sites = f'/normal_distribution_data/normal_distribution_rdt0m_sum_sites_step{special_step_distribution_sum}.npy'\n",
    "#     if os.path.exists(current_path + data_file_name_P):\n",
    "#         P = np.load(current_path + data_file_name_P)\n",
    "#         sites = np.load(current_path + data_file_name_sites)\n",
    "#     else:\n",
    "#         newplace = np.rot90(npy_file_aniso_sum[special_step_distribution_sum,:,:,:], 1, (0,1))\n",
    "#         P, sites, sites_list = get_normal_vector(newplace, initial_grain_num)\n",
    "#         np.save(current_path + data_file_name_P, P)\n",
    "#         np.save(current_path + data_file_name_sites, sites)\n",
    "\n",
    "#     slope_list = get_normal_vector_slope(P, sites, special_step_distribution_sum, \"Sum\",slope_list_bias)\n",
    "#     aniso_mag[3], aniso_mag_stand[3] = simple_magnitude(slope_list)\n",
    "\n",
    "#     # Aniso - consMin\n",
    "#     data_file_name_P = f'/normal_distribution_data/normal_distribution_rdt0m_consMin_P_step{special_step_distribution_consMin}.npy'\n",
    "#     data_file_name_sites = f'/normal_distribution_data/normal_distribution_rdt0m_consMin_P_sites_step{special_step_distribution_consMin}.npy'\n",
    "#     if os.path.exists(current_path + data_file_name_P):\n",
    "#         P = np.load(current_path + data_file_name_P)\n",
    "#         sites = np.load(current_path + data_file_name_sites)\n",
    "#     else:\n",
    "#         newplace = np.rot90(npy_file_aniso_consMin[special_step_distribution_consMin,:,:,:], 1, (0,1))\n",
    "#         P, sites, sites_list = get_normal_vector(newplace, initial_grain_num)\n",
    "#         np.save(current_path + data_file_name_P, P)\n",
    "#         np.save(current_path + data_file_name_sites, sites)\n",
    "\n",
    "#     slope_list = get_normal_vector_slope(P, sites, special_step_distribution_consMin, \"CMin\",slope_list_bias)\n",
    "#     aniso_mag[4], aniso_mag_stand[4] = simple_magnitude(slope_list)\n",
    "\n",
    "#     # Aniso - consMax\n",
    "#     data_file_name_P = f'/normal_distribution_data/normal_distribution_rdt0m_consMax_P_step{special_step_distribution_consMax}.npy'\n",
    "#     data_file_name_sites = f'/normal_distribution_data/normal_distribution_rdt0m_consMax_sites_step{special_step_distribution_consMax}.npy'\n",
    "#     if os.path.exists(current_path + data_file_name_P):\n",
    "#         P = np.load(current_path + data_file_name_P)\n",
    "#         sites = np.load(current_path + data_file_name_sites)\n",
    "#     else:\n",
    "#         newplace = np.rot90(npy_file_aniso_consMax[special_step_distribution_consMax,:,:,:], 1, (0,1))\n",
    "#         P, sites, sites_list = get_normal_vector(newplace, initial_grain_num)\n",
    "#         np.save(current_path + data_file_name_P, P)\n",
    "#         np.save(current_path + data_file_name_sites, sites)\n",
    "\n",
    "#     slope_list = get_normal_vector_slope(P, sites, special_step_distribution_consMax, \"CMax\",slope_list_bias)\n",
    "#     aniso_mag[5], aniso_mag_stand[5] = simple_magnitude(slope_list)\n",
    "\n",
    "    # Aniso - iso\n",
    "    data_file_name_P = f'/normal_distribution_data/normal_distribution_iso_P_step{special_step_distribution_iso}.npy'\n",
    "    data_file_name_sites = f'/normal_distribution_data/normal_distribution_iso_sites_step{special_step_distribution_iso}.npy'\n",
    "    if os.path.exists(current_path + data_file_name_P):\n",
    "        P = np.load(current_path + data_file_name_P)\n",
    "        sites = np.load(current_path + data_file_name_sites)\n",
    "    else:\n",
    "        newplace = np.rot90(npy_file_iso[special_step_distribution_iso,:,:,:], 1, (0,1))\n",
    "        P, sites, sites_list = get_normal_vector(newplace, initial_grain_num)\n",
    "        np.save(current_path + data_file_name_P, P)\n",
    "        np.save(current_path + data_file_name_sites, sites)\n",
    "\n",
    "    slope_list = get_normal_vector_slope(P, sites, special_step_distribution_iso, \"Iso\",slope_list_bias)\n",
    "\n",
    "    plt.legend(loc=(-0.12,-0.35),fontsize=16,ncol=3)\n",
    "    plt.savefig(current_path + \"/figures/normal_distribution_poly_randomtheta0_20k_after_removing_bias.png\", dpi=400,bbox_inches='tight')\n",
    "\n",
    "    # plt.close()\n",
    "    # fig = plt.figure(figsize=(5, 5))\n",
    "    # label_list = [\"Min\", \"Max\", \"Ave\", \"Sum\", \"CMin\", \"CMax\"]\n",
    "    # # plt.errorbar(np.linspace(0,len(label_list)-1,len(label_list)), aniso_mag, yerr=aniso_mag_stand, linestyle='None', marker='None',color='black',linewidth=1, capsize=2)\n",
    "    # plt.plot(np.linspace(0,len(label_list)-1,len(label_list)), aniso_mag, '.-', markersize=8, label='around 2000 grains', linewidth=2)\n",
    "    # plt.xlabel(\"TJ energy approach\", fontsize=16)\n",
    "    # plt.ylabel(\"Anisotropic Magnitude\", fontsize=16)\n",
    "    # plt.xticks([0,1,2,3,4,5],label_list)\n",
    "    # # plt.legend(fontsize=16)\n",
    "    # plt.ylim([-0.05,1.0])\n",
    "    # plt.xticks(fontsize=16)\n",
    "    # plt.yticks(fontsize=16)\n",
    "    # plt.savefig(current_path + \"/figures/anisotropic_polym_randomtheta0_20k_magnitude_polar_ave.png\", dpi=400,bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UFRC Python-3.8",
   "language": "python",
   "name": "python3-3.8-ufrc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
